---
title: "Evaluating Regression Models using Cross Validation and Bootstrap"
author: "Setiawan"
date: "November 25, 2018"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Linear Regression
Let the response Y be Distopia, and consider the numeric variables Economy, Family, Health, Freedom, Trust, and Generosity as the predictor variables; call these x1,x2,...,x6, respectively. For the purpose of predicting the value of Y , we will use linear regression models of the form 
yi = β0 + β1x1i + β2x2i +···+ β6x6i + Ei 
where the Ei are IID N(0,σ2), i = 1,2,...,n.

We will use Cross Validation and Bootstrap methods to evaluate the regression model.

### Importing Data
``` {r data}
library(glmnet)
library(boot)
dta <- read.csv("happiness.csv")
colnames(dta) <- c("Country", "Region", "Religion", "Religious", "Economy",
                   "Family", "Health", "Freedom", "Trust", "Generosity", "Dystopia")
dta=dta[,5:11]
summary(dta)
```

### Cross Validation
In this case we will use leave one out cross validation (LOOCV). It is a K-fold cross validation taken to its logical extreme, with K equal to N, the number of data points in the set. That means that N separate times, the function approximator is trained on all the data except for one point and a prediction is made for that point. As before the average error is computed and used to evaluate the model.

``` {r loocv}
err2=matrix(nrow=157,ncol=1)
for (i in 1:157) {
  dta2=dta[-i,]
  lm=lm(dta2$Dystopia~dta2$Economy+dta2$Family+dta2$Health+dta2$Freedom+dta2$Trust+dta2$Generosity)
  data=dta[i,]
  y_hat=lm$coefficients[1]+lm$coefficients[2]*data[,1]+lm$coefficients[3]*data[,2]+lm$coefficients[4]*data[,3]+lm$coefficients[5]*data[,4]+lm$coefficients[6]*data[,5]+lm$coefficients[7]*data[,6]
  y=data[,7]
  err2[i,]=(y-y_hat)^2
}
cv=1/157*sum(err2)
cat('The Cross Validation estimate of mean squared error is: ',cv)
```

### Bootstrap
The bootstrap is a computational resampling technique for finding standard errors (and in fact other things such as confidence intervals), with the only input being the procedure for calculating the estimate (or estimator) of interest on a sample of data.

``` {r boot}
boot.fn=function(data,index)
  return(coef(lm(Dystopia~Economy+Family+Health+Freedom+Trust+Generosity,data=data,subset=index)))
boot.fn(dta,1:157)
set.seed(1)
B=1000
alpha=matrix(nrow=B, ncol=7)
for (i in 1:B) {
  alpha[i,]=boot.fn(dta,sample(157,157,replace=T))
}
SE=matrix(nrow=1,ncol=7)
original=matrix(nrow=1,ncol=7)
for (i in 1:7) {
  SE[,i]=sd(alpha[,i])
  original[,i]=mean(alpha[,i])
}
boot(dta,boot.fn,B)
original
cat('Standard Error: ', SE)
```

## Lasso Regression
Lasso stands for Least Absolute Shrinkage and Selection Operator. Lasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of regression is well-suited for models showing high levels of muticollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination.

``` {r lasso}
x=model.matrix(Dystopia~.,dta)[,-1]
y=dta$Dystopia
x2 <- scale(x)

set.seed(10)
train=sample(1:nrow(x2), nrow(x2)/2)
test=(-train)
y.test=y[test]

lasso.mod=glmnet(x2[train,],y[train],alpha=1)
plot(lasso.mod,xvar="lambda")

cv.out=cv.glmnet(x2[train,],y[train],alpha=1)
cv.out$lambda.min
plot(cv.out)

lasso.mod=glmnet(x2[test,],y[test],alpha=1,lambda=cv.out$lambda.min)
lasso.pred=predict(lasso.mod,newx=x2[test,])
mean((lasso.pred-y.test)^2)

out=glmnet (x2,y,alpha =1, lambda =cv.out$lambda.min)
lasso.coef=predict (out ,type ="coefficients",s=cv.out$lambda.min )[1:7 ,]
lasso.coef
lm=lm(dta$Dystopia~dta$Economy+dta$Family+dta$Health+dta$Freedom+dta$Trust+dta$Generosity)
lm$coefficients
```


The selected value of lambda is 0.072. Note that, since the value of lambda with the lowest MSE is the largest one tried, we may want to specify the range of lambda to use rather than letting R choose for us. With the selected lambda, all the coefficients are shrunk to zero. Thus, we can apparently improve our predictive accuracy by ignoring all the predictor variables and using an overall mean from the training set as our prediction for every individual.